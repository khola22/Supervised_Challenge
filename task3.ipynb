{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3b056460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from helper import evaluate_model, target_encode\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3951635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/tracks_spectral_reduced.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83ad3bf",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c3a845",
   "metadata": {},
   "source": [
    "Since we will apply regression models in this part, we have to handle categorical columns. We will either encode them or drop them if they have high cardinality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "50e35dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['title']\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical columns excluding 'genres' and 'genres_all'\n",
    "# These columns should be dropped or encoded if necessary\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "cols_to_handle = [col for col in categorical_cols if col not in ['genres', 'genres_all','artist_name','album_title']]\n",
    "print(cols_to_handle)\n",
    "# ['title']\n",
    "# Since these columns have high cardinality and are not useful for regression, we will drop them\n",
    "cols_to_drop = cols_to_handle\n",
    "df = df.drop(columns=cols_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfca5376",
   "metadata": {},
   "source": [
    "Now, we will analyse the target feature : duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "86537783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    97288.00\n",
      "mean       274.82\n",
      "std        283.36\n",
      "min          0.00\n",
      "25%        151.00\n",
      "50%        218.00\n",
      "75%        306.00\n",
      "max      11030.00\n",
      "Name: duration, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Outlier clipping and log transformation for 'duration'\n",
    "print(df['duration'].describe().round(2).T)\n",
    "\n",
    "# We see an extreme outlier at 11030 seconds, so we will clip durations at 2000 seconds.\n",
    "\n",
    "# Clipping\n",
    "MAX_DURATION = 600.0\n",
    "df['duration_clipped'] = np.clip(df['duration'], a_min=None, a_max=MAX_DURATION)\n",
    "\n",
    "# log transformation (to reduce skewness for regression)\n",
    "df['y_duration_log'] = np.log1p(df['duration_clipped'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7879cf39",
   "metadata": {},
   "source": [
    "Now, we define the input set and the target 'y_duration_log'.  \n",
    "For the input, it will exclude the target feature (and its origins, 'duration' and 'duration_clipped'). It will exclude genres and genres_all too because they are a list of ids. Finally, it will exclude the columns 'Unnamed: 0' and the column 'track_id'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "392948ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X (Input Features): (97288, 21)\n",
      "Shape of Y (Target): (97288,)\n"
     ]
    }
   ],
   "source": [
    "# Split into inputs X and target Y\n",
    "Y = df['y_duration_log']\n",
    "cols_to_exclude = [\n",
    "    'duration',\n",
    "    'duration_clipped',\n",
    "    'y_duration_log',\n",
    "    'genres', 'genres_all',\n",
    "    'Unnamed: 0',\n",
    "    'track_id'\n",
    "]\n",
    "X = df.drop(columns=[col for col in cols_to_exclude if col in df.columns], errors='ignore')\n",
    "print(f\"Shape of X (Input Features): {X.shape}\")\n",
    "print(f\"Shape of Y (Target): {Y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca30def5",
   "metadata": {},
   "source": [
    "Here, we split the data into train and test data and we standarize based on X_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e4894632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (77830, 21)\n",
      "Shape of X_test: (19458, 21)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, \n",
    "    test_size=0.2, \n",
    "    random_state=42 \n",
    ")\n",
    "\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c83fd1",
   "metadata": {},
   "source": [
    "We then need to make some changes regarding the names of the artists and the album titles. We can't apply hot encoding on these specific features because of their high cardinality, but we can identify each artist/album with their average duration time. We apply the target encoding. This allows us to turn a high-cardinality categorical feature into a single, powerful numerical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a874a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_encode = ['artist_name', 'album_title']\n",
    "cols_to_encode = [c for c in cols_to_encode if c in X_train.columns]\n",
    "\n",
    "for col in cols_to_encode:\n",
    "    smoothing = 5 if col == 'album_title' else 10\n",
    "    X_train, X_test = target_encode(X_train, X_test, Y_train, col, m=smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "867cd447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f11842",
   "metadata": {},
   "source": [
    "## Baseline : KNN Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8586af5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of KNN Regressor\n",
      "R² Score: 0.4470\n",
      "RMSE (Log-Seconds): 0.5009\n",
      "MAE (Log-Seconds): 0.3624\n",
      "MAE (Seconds): 80.95 seconds\n"
     ]
    }
   ],
   "source": [
    "# Baseline : KNN Regressor\n",
    "\n",
    "model_knn = KNeighborsRegressor(n_neighbors=5) \n",
    "model_knn.fit(X_train_scaled, Y_train)\n",
    "\n",
    "print(\"Performance of KNN Regressor\")\n",
    "#predict and evaluate\n",
    "evaluate_model(model_knn, X_test_scaled, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f08796",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5365a569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of Random Forest Regressor\n",
      "R² Score: 0.5044\n",
      "RMSE (Log-Seconds): 0.4741\n",
      "MAE (Log-Seconds): 0.3367\n",
      "MAE (Seconds): 74.58 seconds\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Regressor\n",
    "model_rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "model_rf.fit(X_train_scaled, Y_train)\n",
    "\n",
    "print(\"Performance of Random Forest Regressor\")\n",
    "#predict and evaluate\n",
    "evaluate_model(model_rf, X_test_scaled, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4ed28b",
   "metadata": {},
   "source": [
    "## Merge with Echonest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dab795",
   "metadata": {},
   "source": [
    "In this part, we will add features from `echonest_features.tsv`. This merge will reduce the size of the dataset to 90%.  \n",
    "We will compare the performance of a random forest regressor on a small data set with more features and on a large data set but with less features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e1cffe07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old shape : (97288, 28)\n",
      "New shape (after merge) : (10552, 36)\n"
     ]
    }
   ],
   "source": [
    "# Merging with Echonest features\n",
    "echonest = pd.read_csv('data/echonest_features.tsv', sep='\\t')\n",
    "\n",
    "df_complet = df.merge(echonest, on='track_id')\n",
    "\n",
    "print(f\"Old shape : {df.shape}\")\n",
    "print(f\"New shape (after merge) : {df_complet.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0160993a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "categorical_cols_after = df_complet.select_dtypes(include=['object']).columns.tolist()\n",
    "cols_to_handle_after = [col for col in categorical_cols_after if col not in ['genres', 'genres_all','artist_name','album_title']]\n",
    "print(cols_to_handle_after)\n",
    "# No new categorical columns introduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5db39b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                   0.000000\n",
       "track_id                     0.000000\n",
       "album_title                  0.000000\n",
       "album_tracks                 0.000000\n",
       "artist_latitude              0.000000\n",
       "artist_longitude             0.000000\n",
       "artist_name                  0.000000\n",
       "duration                     0.000000\n",
       "genre_top                    0.000000\n",
       "genres                       0.000000\n",
       "genres_all                   0.000000\n",
       "spectral_bandwidth_max_01    0.000000\n",
       "spectral_bandwidth_min_01    0.000000\n",
       "spectral_bandwidth_std_01    0.000000\n",
       "spectral_centroid_max_01     0.000000\n",
       "spectral_centroid_min_01     0.000000\n",
       "spectral_centroid_std_01     0.000000\n",
       "spectral_rolloff_max_01      0.000000\n",
       "spectral_rolloff_min_01      0.000000\n",
       "spectral_rolloff_std_01      0.000000\n",
       "artist_location_unknown      0.000000\n",
       "g4_pc1                       0.000000\n",
       "g5_pc1                       0.000000\n",
       "g6_pc1                       0.000000\n",
       "g6_pc2                       0.000000\n",
       "g6_pc3                       0.000000\n",
       "duration_clipped             0.000000\n",
       "y_duration_log               0.000000\n",
       "acousticness                 0.000000\n",
       "danceability                 0.180061\n",
       "energy                       0.000000\n",
       "instrumentalness             0.000000\n",
       "liveness                     0.000000\n",
       "speechiness                  1.374147\n",
       "tempo                        0.000000\n",
       "valence                      0.199014\n",
       "dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verifying missing values after merging\n",
    "df_complet.isnull().sum()/len(df_complet) *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b1ed8f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values with median\n",
    "cols_to_impute = ['speechiness', 'valence', 'danceability']\n",
    "\n",
    "for col in cols_to_impute:\n",
    "    median_val = df_complet[col].median()\n",
    "    df_complet[col] = df_complet[col].fillna(median_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "60cf28b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data after merging with Echonest features\n",
    "X_EXCLUDE = [\n",
    "    'Unnamed: 0', \n",
    "    'track_id',\n",
    "    'duration',           \n",
    "    'duration_clipped',   \n",
    "    'y_duration_log',     \n",
    "    'genres',             \n",
    "    'genres_all'          \n",
    "]\n",
    "\n",
    "Y1 = df_complet['y_duration_log']\n",
    "X1 = df_complet.drop(columns=[col for col in X_EXCLUDE if col in df_complet.columns], errors='ignore')\n",
    "\n",
    "X_train1, X_test1, Y_train1, Y_test1 = train_test_split(X1, Y1, test_size=0.2, random_state=42)\n",
    "\n",
    "cols_to_encode1 = ['artist_name', 'album_title']\n",
    "cols_to_encode1 = [c for c in cols_to_encode if c in X_train1.columns]\n",
    "\n",
    "for col in cols_to_encode1:\n",
    "    smoothing = 5 if col == 'album_title' else 10\n",
    "    X_train1, X_test1 = target_encode(X_train1, X_test1, Y_train1, col, m=smoothing)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled1 = scaler.fit_transform(X_train1)\n",
    "X_test_scaled1 = scaler.transform(X_test1)\n",
    "X_train_scaled1 = pd.DataFrame(X_train_scaled1, columns=X_train1.columns, index=X_train1.index)\n",
    "X_test_scaled1 = pd.DataFrame(X_test_scaled1, columns=X_test1.columns, index=X_test1.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6695fd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Random Forest Regressor after merging with Echonest features:\n",
      "R² Score: 0.3853\n",
      "RMSE (Log-Seconds): 0.3824\n",
      "MAE (Log-Seconds): 0.2801\n",
      "MAE (Seconds): 61.16 seconds\n"
     ]
    }
   ],
   "source": [
    "# Apply Random Forest Regressor on the new dataset\n",
    "model_rf1 = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "model_rf1.fit(X_train_scaled1, Y_train1)\n",
    "\n",
    "print(\"\\nPerformance Random Forest Regressor after merging with Echonest features:\")\n",
    "\n",
    "# predict and evaluate\n",
    "evaluate_model(model_rf1, X_test_scaled1, Y_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1112732",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8c1c30f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance of Gradient Boosting Regressor on the merged dataset:\n",
      "R² Score: 0.3936\n",
      "RMSE (Log-Seconds): 0.3798\n",
      "MAE (Log-Seconds): 0.2780\n",
      "MAE (Seconds): 60.53 seconds\n"
     ]
    }
   ],
   "source": [
    "model_gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "model_gbr.fit(X_train_scaled1, Y_train1)\n",
    "\n",
    "print(\"\\nPerformance of Gradient Boosting Regressor on the merged dataset:\")\n",
    "# predict and evaluate\n",
    "evaluate_model(model_gbr, X_test_scaled1, Y_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616a6916",
   "metadata": {},
   "source": [
    "### XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0b510d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " XGBoost Regressor\n",
      "R² Score: 0.5247\n",
      "RMSE (Log-Seconds): 0.4644\n",
      "MAE (Log-Seconds): 0.3328\n",
      "MAE (Seconds): 73.87 seconds\n"
     ]
    }
   ],
   "source": [
    "model_xgb = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=10,        # Deeper trees to capture interactions\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\" XGBoost Regressor\")\n",
    "\n",
    "\n",
    "\n",
    "model_xgb.fit(X_train_scaled, Y_train)\n",
    "evaluate_model(model_xgb, X_test_scaled, Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "91eeac0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036971 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4659\n",
      "[LightGBM] [Info] Number of data points in the train set: 77830, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 5.325285\n",
      "R² Score: 0.5245\n",
      "RMSE (Log-Seconds): 0.4644\n",
      "MAE (Log-Seconds): 0.3336\n",
      "MAE (Seconds): 74.05 seconds\n"
     ]
    }
   ],
   "source": [
    "model_lgb = LGBMRegressor(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=-1,\n",
    "    num_leaves=64,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model_lgb.fit(X_train_scaled, Y_train)\n",
    "evaluate_model(model_lgb, X_test_scaled, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
